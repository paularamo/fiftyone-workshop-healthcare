{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5a2fd6",
   "metadata": {},
   "source": [
    "# 🏥 Visual AI in Healthcare with FiftyOne - DeepLesion Exploration/Quering/Filtering\n",
    "**Empowering medical imaging workflows with open-source tools and modern AI**\n",
    "\n",
    "This notebook is part of the **“Visual AI in Healthcare with FiftyOne”** workshop. Through hands-on examples, we explore how to load, visualize, analyze, and enhance medical imaging datasets using state-of-the-art AI tools.\n",
    "\n",
    "🔬 **What you’ll learn in this notebook:**\n",
    "\n",
    "- How to **load the DeepLesion dataset** using the Hugging Face `datasets` library  \n",
    "- How to **explore and assess the quality** of medical imaging data using FiftyOne Brain  \n",
    "- How to **create balanced subsets** for training or evaluation  \n",
    "- How to **push curated datasets** to the Hugging Face Hub for sharing and reuse  \n",
    "- How to **launch the FiftyOne App** for interactive analysis and visualization\n",
    "\n",
    "📚 **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` – Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` – Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` – Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` – Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` – Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` – Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` – Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84dd49",
   "metadata": {},
   "source": [
    "### ✅ Requirements\n",
    "\n",
    "Please install all the requeriments for running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf035127",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets fiftyone pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4606c",
   "metadata": {},
   "source": [
    "### 📥 Import `load_dataset` from Hugging Face to access medical imaging datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"farrell236/DeepLesion\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc649794",
   "metadata": {},
   "source": [
    "### 📊 Import pandas for data manipulation (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e86b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cd6a1",
   "metadata": {},
   "source": [
    "### 🔧 Install the Hugging Face Hub / Using Repo scripts for dataset downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa58888",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install huggingface_hub\n",
    "git lfs install\n",
    "git clone https://huggingface.co/datasets/farrell236/DeepLesion\n",
    "# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/farrell236/DeepLesion\n",
    "cd DeepLesion/scripts\n",
    "python batch_download_zips.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46e5052",
   "metadata": {},
   "source": [
    "### 📂 Import zipfile to handle compressed data formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e427509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set your zip folder and destination\n",
    "zip_folder = \"./DeepLesion/Images_zip\"\n",
    "extract_dir = \"./DeepLesion/deeplesion_images\"\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all zip files\n",
    "zip_files = glob(os.path.join(zip_folder, \"*.zip\"))\n",
    "\n",
    "for zip_path in tqdm(zip_files, desc=\"Extracting and deleting zip files\"):\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)\n",
    "        os.remove(zip_path)  # ✅ Delete zip file after successful extraction\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {zip_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcabf8",
   "metadata": {},
   "source": [
    "### 🏗 Preprocessing DeepLesion CT Scans with DICOM Windowing\n",
    "\n",
    "This section performs intensity normalization on CT scan slices from the DeepLesion dataset. Each image is originally stored as a 16-bit PNG representing Hounsfield Units (HU). To prepare the data for downstream tasks (e.g., visualization, model training), we apply the following preprocessing steps:\n",
    "\n",
    "- **Read DICOM metadata** from the DeepLesion CSV file, including windowing parameters for each scan.\n",
    "- **Clip and normalize intensities** to an 8-bit range \\[0–255\\] using DICOM windowing (typically -150 to 250 HU).\n",
    "- **Preserve folder structure**, writing the processed images into a new directory (`Images_png_wn`) with one subfolder per series.\n",
    "- **Convert all slices in each scan**, transforming them from raw HU values to normalized images suitable for machine learning.\n",
    "\n",
    "This ensures all CT slices are visually and numerically consistent, facilitating better model performance and human interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "dir_in = \"./DeepLesion/deeplesion_images/Images_png\"\n",
    "dir_out = \"./DeepLesion/deeplesion_images/Images_png_wn\"\n",
    "info_fn = \"./DeepLesion/DL_info.csv\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(dir_out, exist_ok=True)\n",
    "\n",
    "# Load metadata\n",
    "dl_info = pd.read_csv(info_fn)\n",
    "\n",
    "def clip_and_normalize(np_image: np.ndarray, clip_min: int = -150, clip_max: int = 250) -> np.ndarray:\n",
    "    \"\"\"Apply intensity windowing and scale to 8-bit\"\"\"\n",
    "    np_image = np.clip(np_image, clip_min, clip_max)\n",
    "    np_image = (np_image - clip_min) / (clip_max - clip_min)\n",
    "    return np_image\n",
    "\n",
    "# Track folders already processed to avoid repeat\n",
    "created_dirs = set()\n",
    "\n",
    "for idx, row in tqdm(dl_info.iterrows(), total=len(dl_info)):\n",
    "    folder = row['File_name'].rsplit('_', 1)[0]  # e.g. 000001_01_01\n",
    "    subdir_in = os.path.join(dir_in, folder)\n",
    "    subdir_out = os.path.join(dir_out, folder)\n",
    "\n",
    "    if not os.path.exists(subdir_out):\n",
    "        os.makedirs(subdir_out, exist_ok=True)\n",
    "\n",
    "    # Get DICOM window values\n",
    "    try:\n",
    "        DICOM_windows = [float(v.strip()) for v in row['DICOM_windows'].split(',')]\n",
    "        clip_min, clip_max = DICOM_windows[0], DICOM_windows[1]\n",
    "    except:\n",
    "        clip_min, clip_max = -150, 250  # default if broken\n",
    "        print(f\"Invalid DICOM window for {row['File_name']}, using default\")\n",
    "\n",
    "    # Process all slices in folder\n",
    "    images = sorted(glob(os.path.join(subdir_in, \"*.png\")))\n",
    "    for im in images:\n",
    "        try:\n",
    "            image = cv2.imread(im, cv2.IMREAD_UNCHANGED)\n",
    "            image = image.astype(\"int32\") - 32768  # Convert to Hounsfield Units\n",
    "            image = clip_and_normalize(image, clip_min, clip_max)\n",
    "            image = (image * 255).astype(\"uint8\")\n",
    "\n",
    "            out_path = os.path.join(subdir_out, os.path.basename(im))\n",
    "            cv2.imwrite(out_path, image)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert {im}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7d58a",
   "metadata": {},
   "source": [
    "### 🧬 Constructing a FiftyOne Dataset from DeepLesion Metadata\n",
    "\n",
    "In this section, we build a structured `fiftyone.Dataset` using the DeepLesion image directory and accompanying metadata from `DL_info.csv`.\n",
    "\n",
    "Each row in the metadata corresponds to a CT slice with rich clinical context, including:\n",
    "- **Patient, study, and series identifiers**\n",
    "- **Bounding boxes** for lesions (if available)\n",
    "- **DICOM metadata** such as windowing, slice range, and spacing\n",
    "- **Patient-level info** like age and gender\n",
    "\n",
    "The workflow:\n",
    "- Locates each image in a subfolder by parsing its filename\n",
    "- Applies any available bounding box annotations, normalized to the \\[0, 1\\] range (FiftyOne format)\n",
    "- Embeds image-level metadata and tags into each `fo.Sample`\n",
    "- Adds the samples to a new dataset called `\"deeplesion_wn\"` (window-normalized version)\n",
    "\n",
    "This enables structured, queryable exploration of DeepLesion with FiftyOne, ready for downstream tasks like model training, evaluation, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b499ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "from fiftyone.core.metadata import ImageMetadata\n",
    "#from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "csv_path = \"./DeepLesion/DL_info.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Load metadata\n",
    "#ds = load_dataset(\"farrell236/DeepLesion\")\n",
    "#df = pd.DataFrame(ds[\"train\"])\n",
    "\n",
    "# Root path to nested folders\n",
    "image_root = \"./DeepLesion/deeplesion_images/Images_png_wn\"\n",
    "\n",
    "# Init FiftyOne dataset\n",
    "dataset = fo.Dataset(\"deeplesion_wn\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    file_name = row[\"File_name\"]  # e.g. 000001_01_01_109.png\n",
    "    parts = file_name.split(\"_\")\n",
    "\n",
    "    # Subfolder = 000001_01_01\n",
    "    subfolder = \"_\".join(parts[:3])\n",
    "\n",
    "    # Slice file = 109\n",
    "    slice_part = parts[3].replace(\".png\", \"\")\n",
    "\n",
    "    # Full image path\n",
    "    img_path = os.path.join(image_root, subfolder, f\"{slice_part}.png\")\n",
    "\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Missing: {img_path}\")\n",
    "    else:\n",
    "        print(f\"Found: {img_path}\")\n",
    "    if not os.path.exists(img_path):\n",
    "        print(f\"Missing: {img_path}\")\n",
    "        continue\n",
    "\n",
    "    # Parse bounding box if available\n",
    "    try:\n",
    "        bbox = list(map(float, row[\"Bounding_boxes\"].split(\", \")))\n",
    "        x, y, w, h = bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        detection = fo.Detection(label=\"lesion\", bounding_box=[x / 512, y / 512, w / 512, h / 512])\n",
    "        detections = [detection]\n",
    "    except:\n",
    "        detections = []\n",
    "\n",
    "    # Create sample with metadata\n",
    "    sample = fo.Sample(\n",
    "        filepath=img_path,\n",
    "        metadata=ImageMetadata(width=512, height=512),\n",
    "        ground_truth=fo.Detections(detections=detections),\n",
    "        patient_index=row[\"Patient_index\"],\n",
    "        study_index=row[\"Study_index\"],\n",
    "        series_id=row[\"Series_ID\"],\n",
    "        key_slice_index=row[\"Key_slice_index\"],\n",
    "        lesion_type=row[\"Coarse_lesion_type\"],\n",
    "        pixel_spacing=row[\"Spacing_mm_px_\"],\n",
    "        dicom_window=row[\"DICOM_windows\"],\n",
    "        age=row[\"Patient_age\"],\n",
    "        gender=row[\"Patient_gender\"],\n",
    "        slice_range=row[\"Slice_range\"],\n",
    "        lesion_diameters=row[\"Lesion_diameters_Pixel_\"],\n",
    "        possibly_noisy=row[\"Possibly_noisy\"],\n",
    "        tags=[\"deeplesion\"]\n",
    ")\n",
    "\n",
    "\n",
    "    dataset.add_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb77943",
   "metadata": {},
   "source": [
    "### 💾 Make the FiftyOne dataset persistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc17e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.persistent=True\n",
    "session = fo.launch_app(dataset, port=5151, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665daed3",
   "metadata": {},
   "source": [
    "### 🧩 Indexing Custom Fields for Efficient Querying\n",
    "\n",
    "FiftyOne allows you to index specific fields within your dataset to enable **faster filtering, searching, and slicing**—especially useful when working with large medical datasets like DeepLesion.\n",
    "\n",
    "In this step, we define a list of important metadata fields (e.g., patient ID, study ID, lesion type, age, etc.) and attempt to create an index on each one. These fields cover both:\n",
    "- **Annotation-level data** (like `ground_truth.detections.label`)\n",
    "- **Clinical metadata** (like `patient_index`, `pixel_spacing`, `gender`, etc.)\n",
    "\n",
    "If a field cannot be indexed (e.g., due to missing values or data type issues), it logs an error but continues indexing the rest.\n",
    "\n",
    "Indexing is critical for enabling high-performance queries and visual exploration in FiftyOne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of custom fields to index\n",
    "fields_to_index = [\n",
    "    \"ground_truth.detections.label\",  # Label name\n",
    "    \"patient_index\",\n",
    "    \"study_index\",\n",
    "    \"series_id\",\n",
    "    \"key_slice_index\",\n",
    "    \"lesion_type\",\n",
    "    \"pixel_spacing\",\n",
    "    \"dicom_window\",\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"slice_range\",\n",
    "    \"lesion_diameters\",\n",
    "    \"possibly_noisy\",\n",
    "]\n",
    "\n",
    "# Create indexes\n",
    "for field in fields_to_index:\n",
    "    try:\n",
    "        dataset.create_index(field)\n",
    "        print(f\"Indexed: {field}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to index {field}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1eef11",
   "metadata": {},
   "source": [
    "### 🧠 Generating Visual Embeddings with a Pretrained ResNet50 Model\n",
    "\n",
    "In this step, we use FiftyOne Brain to compute **image embeddings** for the DeepLesion dataset using a pretrained model from the FiftyOne Model Zoo.\n",
    "\n",
    "Specifically:\n",
    "- We load **ResNet50 trained on ImageNet** (`resnet50-imagenet-torch`) to extract feature vectors for each CT slice.\n",
    "- We compute embeddings for all samples using `fob.compute_visualization()`.\n",
    "- The resulting embeddings are stored under the field `\"resnet50_embedding1\"` and associated with the brain key `\"deeplesion_emb1\"`.\n",
    "\n",
    "These embeddings serve as a compact numerical representation of each image and are essential for:\n",
    "- Visualizing clusters of similar lesions\n",
    "- Detecting anomalies or outliers\n",
    "- Performing semantic search across patient studies\n",
    "\n",
    "This step sets the foundation for **embedding-based exploration and analysis** in the FiftyOne App.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load dataset\n",
    "#dataset = fo.load_dataset(\"deeplesion\")\n",
    "\n",
    "# Use a pretrained model (e.g., ResNet50)\n",
    "model = foz.load_zoo_model(\"resnet50-imagenet-torch\", include_logits=False)\n",
    "\n",
    "# Compute embeddings for all samples\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    model=model,\n",
    "    embeddings=\"resnet50_embedding1\",\n",
    "    brain_key=\"deeplesion_emb1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1e3a6",
   "metadata": {},
   "source": [
    "### 🔍 Identifying the Most Unique CT Slices with FiftyOne Brain\n",
    "\n",
    "In this step, we use **FiftyOne Brain's uniqueness algorithm** to analyze the dataset and assign a `uniqueness` score to each sample. This score reflects how different a sample is compared to others in the embedding space.\n",
    "\n",
    "- `fob.compute_uniqueness(dataset)` computes and stores uniqueness scores for all samples.\n",
    "- We then **sort the dataset by uniqueness** (in descending order) and select the **top 2,000 most unique slices** using `.limit()`.\n",
    "\n",
    "This unique view helps surface rare or unusual cases, which is useful for:\n",
    "- Curating diverse training sets\n",
    "- Spotting labeling or imaging anomalies\n",
    "- Selecting high-value examples for manual review or annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d7630",
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_uniqueness(dataset)\n",
    "# Create a view with the 100 most unique samples\n",
    "unique_view = dataset.sort_by(\"uniqueness\", reverse=True).limit(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c7f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(unique_view, port=5152, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698d108",
   "metadata": {},
   "source": [
    "### ⚖️ Creating a Balanced Subset of DeepLesion by Lesion Size and Type\n",
    "\n",
    "This block builds a curated, balanced subset of the DeepLesion dataset focused on **larger lesions** (short axis > 10mm) and **uniform class distribution** across lesion types.\n",
    "\n",
    "#### 📋 Step-by-step process:\n",
    "\n",
    "1. **Compute short-axis diameters**  \n",
    "   Extracts the smaller of the two lesion diameters from the `lesion_diameters` field using a helper function.\n",
    "\n",
    "2. **Filter lesions with short diameter > 10mm**  \n",
    "   Larger lesions are prioritized for clinical relevance and improved visibility.\n",
    "\n",
    "3. **List all unique lesion types**  \n",
    "   These will serve as the \"classes\" for balancing.\n",
    "\n",
    "4. **Sample uniformly per class**  \n",
    "   Randomly selects up to `250` samples per lesion type (modifiable) to form a class-balanced dataset.\n",
    "\n",
    "5. **Tag and save the subset**  \n",
    "   Uses `dataset.select()` to create a new view and applies the `\"balanced_subset\"` tag for tracking.\n",
    "\n",
    "6. **Launch the FiftyOne App (optional)**  \n",
    "   Opens an interactive session to explore the balanced dataset visually.\n",
    "\n",
    "This balanced subset is ideal for downstream tasks like training detection models or running fine-tuning experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "# dataset = fo.load_dataset(\"deeplesion\")\n",
    "\n",
    "# Step 1: Compute short diameter from lesion_diameters\n",
    "def get_short_diameter(diam_str):\n",
    "    try:\n",
    "        values = list(map(float, diam_str.split(\", \")))\n",
    "        return min(values)  # short diameter\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "short_diameters = {\n",
    "    sample.id: get_short_diameter(sample[\"lesion_diameters\"])\n",
    "    for sample in dataset.select_fields([\"lesion_diameters\"])\n",
    "}\n",
    "\n",
    "dataset.set_values(\"short_diameter\", short_diameters, key_field=\"id\")\n",
    "\n",
    "# Step 2: Filter for lesions > 10mm short axis\n",
    "filtered_view = dataset.match(F(\"short_diameter\") > 10)\n",
    "\n",
    "# Step 3: Determine available lesion types\n",
    "lesion_types = filtered_view.distinct(\"lesion_type\")\n",
    "print(\"Lesion types found:\", lesion_types)\n",
    "\n",
    "# Step 4: Balanced sampling\n",
    "samples_per_class = 250  # Adjust this based on your total budget (e.g., 8 classes * 250 = 2000)\n",
    "balanced_ids = []\n",
    "\n",
    "for lesion_type in lesion_types:\n",
    "    view = filtered_view.match(F(\"lesion_type\") == lesion_type)\n",
    "    sampled_view = view.shuffle(seed=42).take(samples_per_class)\n",
    "    ids = sampled_view.values(\"id\")\n",
    "    balanced_ids.extend(ids)\n",
    "\n",
    "# Step 5: Select and tag balanced view\n",
    "balanced_view = dataset.select(balanced_ids)\n",
    "balanced_view.tag_samples(\"balanced_subset\")\n",
    "print(f\"✅ Total balanced samples: {len(balanced_view)}\")\n",
    "\n",
    "# Step 6: (Optional) Launch FiftyOne app\n",
    "session = fo.launch_app(balanced_view, port=5151, auto=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222796f",
   "metadata": {},
   "source": [
    "### 💾 Cloning and Exporting the Balanced DeepLesion Subset\n",
    "\n",
    "Once the balanced view has been created and validated, we clone it into a new standalone FiftyOne dataset called `\"deeplesion_balanced\"`.\n",
    "\n",
    "This is useful for:\n",
    "- Reusing the dataset independently of the full DeepLesion dataset\n",
    "- Exporting for training or sharing\n",
    "- Ensuring reproducibility\n",
    "\n",
    "We then export the dataset using FiftyOne’s built-in exporter:\n",
    "- The dataset is saved in **FiftyOneDataset format**\n",
    "- It includes all media and labels under the `ground_truth` field\n",
    "- Files are moved (or optionally copied) into a clean export directory\n",
    "- Existing content in the export folder is overwritten for freshness\n",
    "\n",
    "The result is a portable, tagged, and size-filtered dataset ready for downstream modeling or publishing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18432a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the view to a new dataset\n",
    "balanced_dataset = balanced_view.clone(name=\"deeplesion_balanced\")\n",
    "print(f\"✅ Cloned to dataset: {balanced_dataset.name}\")\n",
    "\n",
    "export_dir = \"./deeplesion_balanced_export\"\n",
    "\n",
    "balanced_dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    label_field=\"ground_truth\",         # Include labels if they exist\n",
    "    media_export_policy=\"move\",         # Or \"copy\" if you want to keep originals\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"📁 Exported dataset to: {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(balanced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf19ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "\n",
    "push_to_hub(balanced_dataset, \"deeplesion_balanced_fiftyone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
