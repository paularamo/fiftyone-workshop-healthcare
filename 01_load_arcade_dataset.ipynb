{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cedd252",
   "metadata": {},
   "source": [
    "# üè• Visual AI in Healthcare with FiftyOne - ARCADE Dataset Loading/Exploration\n",
    "**Empowering medical imaging workflows with open-source tools and modern AI**\n",
    "\n",
    "This notebook is part of the **‚ÄúVisual AI in Healthcare with FiftyOne‚Äù** workshop. Through hands-on examples, we explore how to load, visualize, analyze, and enhance medical imaging datasets using state-of-the-art AI tools.\n",
    "\n",
    "üî¨ **What you‚Äôll learn in this notebook:**\n",
    "\n",
    "- How to **load and organize a multi-task medical imaging dataset** (ARCADE) using FiftyOne  \n",
    "- How to **import COCO-style segmentation annotations** for both segmentation and stenosis detection tasks  \n",
    "- How to **tag and enrich samples with metadata** for easier querying and filtering  \n",
    "- How to **merge multiple subsets** into a single persistent FiftyOne dataset  \n",
    "- How to **compute image embeddings** using FiftyOne Brain for exploratory analysis and visualization  \n",
    "- How to **launch the FiftyOne App** for interactive dataset exploration\n",
    "- How to **export your dataser** and **load it to Hugging Face**\n",
    "\n",
    "üìö **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` ‚Äì Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` ‚Äì Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` ‚Äì Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` ‚Äì Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` ‚Äì Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` ‚Äì Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` ‚Äì Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1617b3c",
   "metadata": {},
   "source": [
    "### ‚úÖ Requirements\n",
    "\n",
    "Please install all the requeriments for running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf035127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets fiftyone pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f2347",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Loading and Merging ARCADE Dataset Subsets into FiftyOne\n",
    "\n",
    "The ARCADE challenge includes multiple datasets across two key tasks ‚Äî **segmentation** and **stenosis detection** ‚Äî spread across training, validation, and test phases. This section consolidates all subsets into a single FiftyOne dataset to simplify exploration and analysis. Download the dataset [here](https://zenodo.org/records/10390295)\n",
    "\n",
    "#### üîç What's happening here:\n",
    "\n",
    "- Defines the paths and structure for the **segmentation** and **stenosis** datasets across training, validation, and final test phases.\n",
    "- Initializes a new dataset called `\"arcade_combined\"` and deletes any prior version with the same name.\n",
    "- **Adds custom fields** to the dataset schema:\n",
    "  - `phase` ‚Äì e.g., `phase_1`, `final_phase`\n",
    "  - `task` ‚Äì either `segmentation` or `stenosis`\n",
    "  - `subset_name` ‚Äì specific subset like `seg_train` or `sten_val`\n",
    "- For each subset:\n",
    "  - Loads annotations in COCO format using `COCODetectionDatasetImporter`\n",
    "  - Creates a temporary dataset and populates it with images and labels\n",
    "  - Assigns field values and adds sample-level tags\n",
    "  - Merges the processed samples into the main dataset\n",
    "  - Deletes the temporary dataset after merging\n",
    "\n",
    "This structure allows for **highly flexible querying** (e.g., \"show all stenosis test cases\") and enables rich filtering and visualization workflows in FiftyOne.g\n",
    "\n",
    "This creates a unified dataset in FiftyOne where you can filter by phase, task type, or subset name, setting the foundation for structured medical image analysis and model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "# Base path\n",
    "base_path = \"/path/to//arcade_challenge_datasets\"\n",
    "combined_dataset_name = \"arcade_combined\"\n",
    "\n",
    "# Dataset config\n",
    "datasets = [\n",
    "    (\"seg_train\", \"dataset_phase_1/segmentation_dataset/seg_train\", \"phase_1\", \"segmentation\"),\n",
    "    (\"seg_val\", \"dataset_phase_1/segmentation_dataset/seg_val\", \"phase_1\", \"segmentation\"),\n",
    "    (\"sten_train\", \"dataset_phase_1/stenosis_dataset/sten_train\", \"phase_1\", \"stenosis\"),\n",
    "    (\"sten_val\", \"dataset_phase_1/stenosis_dataset/sten_val\", \"phase_1\", \"stenosis\"),\n",
    "    (\"test_case_seg\", \"dataset_final_phase/test_case_segmentation\", \"final_phase\", \"segmentation\"),\n",
    "    (\"test_case_sten\", \"dataset_final_phase/test_cases_stenosis\", \"final_phase\", \"stenosis\"),\n",
    "]\n",
    "\n",
    "# Delete existing dataset\n",
    "if fo.dataset_exists(combined_dataset_name):\n",
    "    fo.delete_dataset(combined_dataset_name)\n",
    "combined_dataset = fo.Dataset(combined_dataset_name)\n",
    "\n",
    "# Add metadata fields\n",
    "combined_dataset.add_sample_field(\"phase\", fo.StringField)\n",
    "combined_dataset.add_sample_field(\"task\", fo.StringField)\n",
    "combined_dataset.add_sample_field(\"subset_name\", fo.StringField)\n",
    "\n",
    "# Load each dataset separately and assign fields\n",
    "for subset_name, relative_path, phase, task in datasets:\n",
    "    print(f\"\\nüì¶ Loading {subset_name}...\")\n",
    "\n",
    "    image_dir = os.path.join(base_path, relative_path, \"images\")\n",
    "    annotation_dir = os.path.join(base_path, relative_path, \"annotations\")\n",
    "    json_files = [f for f in os.listdir(annotation_dir) if f.endswith(\".json\")]\n",
    "    assert len(json_files) == 1, f\"Expected 1 JSON in {annotation_dir}, found {len(json_files)}\"\n",
    "    labels_path = os.path.join(annotation_dir, json_files[0])\n",
    "\n",
    "    # Create temporary dataset\n",
    "    temp_dataset_name = f\"{combined_dataset_name}_{subset_name}\"\n",
    "    if fo.dataset_exists(temp_dataset_name):\n",
    "        fo.delete_dataset(temp_dataset_name)\n",
    "    temp_dataset = fo.Dataset(temp_dataset_name)\n",
    "\n",
    "    importer = COCODetectionDatasetImporter(\n",
    "        data_path=image_dir,\n",
    "        labels_path=labels_path,\n",
    "        label_types=\"segmentations\",\n",
    "        \n",
    "        include_id=True,\n",
    "        extra_attrs=True,\n",
    "    )\n",
    "\n",
    "    # Add to temp dataset\n",
    "    temp_dataset.add_importer(importer)\n",
    "\n",
    "    # Tag + assign fields\n",
    "    for sample in temp_dataset:\n",
    "        sample[\"phase\"] = phase\n",
    "        sample[\"task\"] = task\n",
    "        sample[\"subset_name\"] = subset_name\n",
    "        sample.tags.extend([subset_name, task, phase])\n",
    "        sample.save()\n",
    "        combined_dataset.add_sample(sample)\n",
    "\n",
    "    # Delete temp dataset\n",
    "    temp_dataset.delete()\n",
    "\n",
    "print(\"\\n‚úÖ All subsets imported successfully!\")\n",
    "\n",
    "# Optional: launch FiftyOne app\n",
    "# fo.launch_app(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156f39d",
   "metadata": {},
   "source": [
    "### üß† Computing and Visualizing Embeddings with FiftyOne Brain\n",
    "\n",
    "Once the full ARCADE dataset has been assembled, this section applies **embedding generation and visualization** techniques using FiftyOne Brain. Embeddings are vector representations of images or patches that capture visual similarity and semantic content, enabling advanced tasks like clustering, outlier detection, and semantic search.\n",
    "\n",
    "#### üóÇ Step 1: Persist and Launch the App\n",
    "- We make the dataset **persistent** so it can be reloaded later.\n",
    "- The FiftyOne App is launched for interactive exploration of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.persistent=True\n",
    "session = fo.launch_app(combined_dataset, port=5153, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628dce65",
   "metadata": {},
   "source": [
    "#### üß† Step 2: Compute Embeddings for Entire Images\n",
    "- Uses FiftyOne Brain‚Äôs `compute_visualization()` to generate **default embeddings** for each image.\n",
    "- These are stored under the `\"default_embedding\"` key and indexed using the `\"arcade_emb\"` brain key.\n",
    "- This allows for visualizing the full dataset in **2D latent space** (e.g., t-SNE or UMAP projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load dataset\n",
    "#dataset = fo.load_dataset(\"deeplesion\")\n",
    "\n",
    "# Compute embeddings for all samples\n",
    "fob.compute_visualization(\n",
    "    combined_dataset,\n",
    "    embeddings=\"default_embedding\",\n",
    "    brain_key=\"arcade_emb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71718b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4759ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac60f4",
   "metadata": {},
   "source": [
    "#### üß© Step 3: Compute Patch-Level Embeddings (Optional)\n",
    "- Instead of entire images, we compute embeddings for **regions of interest**, such as segmentations.\n",
    "- Specifies the patch field (`\"segmentations\"`) to extract ROIs.\n",
    "- Loads a pretrained model (`mobilenet-v2-imagenet-torch`) from the FiftyOne Model Zoo.\n",
    "- Computes embeddings for each patch and stores them under `\"patches_embedding\"`, with a new brain key `\"patches_emb\"`.\n",
    "\n",
    "These visual embeddings enable deep visual exploration and analysis of ARCADE‚Äôs segmentation and stenosis data, surfacing patterns and anomalies that go beyond raw pixel inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the field containing the patches (e.g., detections)\n",
    "patches_field = \"segmentations\"\n",
    "\n",
    "# Option 1: Use a pre-trained model from the FiftyOne Model Zoo\n",
    "model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "\n",
    "# Compute embeddings for the patches using the specified model\n",
    "results = fob.compute_visualization(\n",
    "    combined_dataset, \n",
    "    patches_field=patches_field, \n",
    "    model=model, \n",
    "    embeddings=\"patches_embedding\", \n",
    "    brain_key=\"patches_emb\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c2890",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.reload()\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e191c",
   "metadata": {},
   "source": [
    "### üóÑÔ∏è Exporting the Combined ARCADE Dataset and Publishing to Hugging Face ü§ó\n",
    "\n",
    "This final step packages the merged ARCADE dataset into a shareable format and uploads it to the Hugging Face Hub.\n",
    "\n",
    "#### üöö Step 1: Clone the Dataset\n",
    "We create a **clone** of the `arcade_combined` dataset to keep the export operation clean and isolated. This cloned dataset is named `arcade_combined_export`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999433db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the dataset\n",
    "cloned_dataset = combined_dataset.clone(name=\"arcade_combined_export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee64010",
   "metadata": {},
   "source": [
    "#### üíæ Step 2: Export to Local Directory\n",
    "Using `FiftyOneDataset` format, the cloned dataset is exported to a directory (`./arcade_combined_fiftyone`). This includes:\n",
    "- Image files\n",
    "- COCO-style annotations (in this case, from the `\"segmentations\"` field)\n",
    "- Dataset metadata\n",
    "\n",
    "The `overwrite=True` flag ensures any existing content in the folder is safely replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b11d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set export directory\n",
    "export_dir = \"./arcade_combined_fiftyone\"\n",
    "\n",
    "# Export in FiftyOne format\n",
    "cloned_dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    label_field=\"segmentations\",  # or other label field you want to export\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72e57c",
   "metadata": {},
   "source": [
    "#### ü§ó Step 3: Push to Hugging Face Hub\n",
    "With just one line of code, we use `push_to_hub()` to upload the dataset directly to the Hugging Face Hub. The dataset will appear under the user or organization space defined by the credentials used.\n",
    "\n",
    "Publishing to Hugging Face allows others to:\n",
    "- **Easily reuse your dataset** in notebooks, apps, or models\n",
    "- **View it through the FiftyOne Web App integration**\n",
    "- **Promote reproducibility and collaboration** in open-source healthcare AI research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee2d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "\n",
    "push_to_hub(cloned_dataset, \"arcade_fiftyone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
