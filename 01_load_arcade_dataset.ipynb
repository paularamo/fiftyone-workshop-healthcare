{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cedd252",
   "metadata": {},
   "source": [
    "# 🏥 Visual AI in Healthcare with FiftyOne - ARCADE Dataset Loading/Exploration\n",
    "**Empowering medical imaging workflows with open-source tools and modern AI**\n",
    "\n",
    "This notebook is part of the **“Visual AI in Healthcare with FiftyOne”** workshop. Through hands-on examples, we explore how to load, visualize, analyze, and enhance medical imaging datasets using state-of-the-art AI tools.\n",
    "\n",
    "🔬 **What you’ll learn in this notebook:**\n",
    "\n",
    "- How to **load and organize a multi-task medical imaging dataset** (ARCADE) using FiftyOne  \n",
    "- How to **import COCO-style segmentation annotations** for both segmentation and stenosis detection tasks  \n",
    "- How to **tag and enrich samples with metadata** for easier querying and filtering  \n",
    "- How to **merge multiple subsets** into a single persistent FiftyOne dataset  \n",
    "- How to **compute image embeddings** using FiftyOne Brain for exploratory analysis and visualization  \n",
    "- How to **launch the FiftyOne App** for interactive dataset exploration\n",
    "- How to **export your dataser** and **load it to Hugging Face**\n",
    "\n",
    "📚 **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` – Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` – Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` – Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` – Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` – Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` – Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` – Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1617b3c",
   "metadata": {},
   "source": [
    "### ✅ Requirements\n",
    "\n",
    "Please install all the requeriments for running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf035127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./health_care/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: fiftyone in ./health_care/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in ./health_care/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in ./health_care/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./health_care/lib/python3.10/site-packages (from datasets) (2.2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./health_care/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./health_care/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./health_care/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./health_care/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./health_care/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./health_care/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./health_care/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in ./health_care/lib/python3.10/site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in ./health_care/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./health_care/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./health_care/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiofiles in ./health_care/lib/python3.10/site-packages (from fiftyone) (24.1.0)\n",
      "Requirement already satisfied: argcomplete in ./health_care/lib/python3.10/site-packages (from fiftyone) (3.6.2)\n",
      "Requirement already satisfied: async_lru>=2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (2.0.5)\n",
      "Requirement already satisfied: beautifulsoup4 in ./health_care/lib/python3.10/site-packages (from fiftyone) (4.13.4)\n",
      "Requirement already satisfied: boto3 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.38.41)\n",
      "Requirement already satisfied: cachetools in ./health_care/lib/python3.10/site-packages (from fiftyone) (6.1.0)\n",
      "Requirement already satisfied: dacite<1.8.0,>=1.6.0 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.7.0)\n",
      "Requirement already satisfied: Deprecated in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.2.18)\n",
      "Requirement already satisfied: ftfy in ./health_care/lib/python3.10/site-packages (from fiftyone) (6.3.1)\n",
      "Requirement already satisfied: humanize in ./health_care/lib/python3.10/site-packages (from fiftyone) (4.12.3)\n",
      "Requirement already satisfied: hypercorn>=0.13.2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.17.3)\n",
      "Requirement already satisfied: Jinja2>=3 in ./health_care/lib/python3.10/site-packages (from fiftyone) (3.1.6)\n",
      "Requirement already satisfied: kaleido!=0.2.1.post1 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.0.0)\n",
      "Requirement already satisfied: matplotlib in ./health_care/lib/python3.10/site-packages (from fiftyone) (3.10.3)\n",
      "Requirement already satisfied: mongoengine~=0.29.1 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.29.1)\n",
      "Requirement already satisfied: motor~=3.6.0 in ./health_care/lib/python3.10/site-packages (from fiftyone) (3.6.1)\n",
      "Requirement already satisfied: Pillow>=6.2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (11.2.1)\n",
      "Requirement already satisfied: plotly>=4.14 in ./health_care/lib/python3.10/site-packages (from fiftyone) (6.1.2)\n",
      "Requirement already satisfied: pprintpp in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.4.0)\n",
      "Requirement already satisfied: psutil in ./health_care/lib/python3.10/site-packages (from fiftyone) (7.0.0)\n",
      "Requirement already satisfied: pymongo~=4.9.2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (4.9.2)\n",
      "Requirement already satisfied: pytz in ./health_care/lib/python3.10/site-packages (from fiftyone) (2025.2)\n",
      "Requirement already satisfied: regex in ./health_care/lib/python3.10/site-packages (from fiftyone) (2024.11.6)\n",
      "Requirement already satisfied: retrying in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.3.4)\n",
      "Requirement already satisfied: rtree in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.7.0)\n",
      "Requirement already satisfied: scikit-image in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.25.2)\n",
      "Requirement already satisfied: scipy in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.15.3)\n",
      "Requirement already satisfied: setuptools in ./health_care/lib/python3.10/site-packages (from fiftyone) (78.1.0)\n",
      "Requirement already satisfied: sseclient-py<2,>=1.7.2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.8.0)\n",
      "Requirement already satisfied: sse-starlette<1,>=0.10.3 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.10.3)\n",
      "Requirement already satisfied: starlette>=0.24.0 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.47.1)\n",
      "Requirement already satisfied: strawberry-graphql>=0.262.4 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.275.0)\n",
      "Requirement already satisfied: tabulate in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.9.0)\n",
      "Requirement already satisfied: xmltodict in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.14.2)\n",
      "Requirement already satisfied: universal-analytics-python3<2,>=1.0.1 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.1.1)\n",
      "Requirement already satisfied: pydash in ./health_care/lib/python3.10/site-packages (from fiftyone) (8.0.5)\n",
      "Requirement already satisfied: fiftyone-brain<0.22,>=0.21.2 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.21.2)\n",
      "Requirement already satisfied: fiftyone-db<2.0,>=0.4 in ./health_care/lib/python3.10/site-packages (from fiftyone) (1.1.7)\n",
      "Requirement already satisfied: voxel51-eta<0.15,>=0.14.0 in ./health_care/lib/python3.10/site-packages (from fiftyone) (0.14.2)\n",
      "Requirement already satisfied: opencv-python-headless in ./health_care/lib/python3.10/site-packages (from fiftyone) (4.11.0.86)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./health_care/lib/python3.10/site-packages (from pymongo~=4.9.2->fiftyone) (2.7.0)\n",
      "Requirement already satisfied: httpx>=0.10.0 in ./health_care/lib/python3.10/site-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.28.1)\n",
      "Requirement already satisfied: future in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.0)\n",
      "Requirement already satisfied: glob2 in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (0.7)\n",
      "Requirement already satisfied: jsonlines in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (4.0.0)\n",
      "Requirement already satisfied: py7zr in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.9.0.post0)\n",
      "Requirement already satisfied: rarfile in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (4.2)\n",
      "Requirement already satisfied: six in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (1.17.0)\n",
      "Requirement already satisfied: sortedcontainers in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.4.0)\n",
      "Requirement already satisfied: tzlocal in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (5.3.1)\n",
      "Requirement already satisfied: urllib3 in ./health_care/lib/python3.10/site-packages (from voxel51-eta<0.15,>=0.14.0->fiftyone) (2.5.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./health_care/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./health_care/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in ./health_care/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./health_care/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: anyio in ./health_care/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./health_care/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in ./health_care/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./health_care/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in ./health_care/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: exceptiongroup>=1.1.0 in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (1.3.0)\n",
      "Requirement already satisfied: h2>=3.1.0 in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (4.2.0)\n",
      "Requirement already satisfied: priority in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\n",
      "Requirement already satisfied: taskgroup in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (0.2.2)\n",
      "Requirement already satisfied: tomli in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.2.1)\n",
      "Requirement already satisfied: wsproto>=0.14.0 in ./health_care/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in ./health_care/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in ./health_care/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./health_care/lib/python3.10/site-packages (from Jinja2>=3->fiftyone) (3.0.2)\n",
      "Requirement already satisfied: choreographer>=1.0.5 in ./health_care/lib/python3.10/site-packages (from kaleido!=0.2.1.post1->fiftyone) (1.0.9)\n",
      "Requirement already satisfied: logistro>=1.0.8 in ./health_care/lib/python3.10/site-packages (from kaleido!=0.2.1.post1->fiftyone) (1.1.0)\n",
      "Requirement already satisfied: orjson>=3.10.15 in ./health_care/lib/python3.10/site-packages (from kaleido!=0.2.1.post1->fiftyone) (3.10.18)\n",
      "Requirement already satisfied: simplejson>=3.19.3 in ./health_care/lib/python3.10/site-packages (from choreographer>=1.0.5->kaleido!=0.2.1.post1->fiftyone) (3.20.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./health_care/lib/python3.10/site-packages (from plotly>=4.14->fiftyone) (1.43.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./health_care/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./health_care/lib/python3.10/site-packages (from anyio->httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.3.1)\n",
      "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in ./health_care/lib/python3.10/site-packages (from strawberry-graphql>=0.262.4->fiftyone) (3.2.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./health_care/lib/python3.10/site-packages (from beautifulsoup4->fiftyone) (2.7)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.41 in ./health_care/lib/python3.10/site-packages (from boto3->fiftyone) (1.38.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./health_care/lib/python3.10/site-packages (from boto3->fiftyone) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in ./health_care/lib/python3.10/site-packages (from boto3->fiftyone) (0.13.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./health_care/lib/python3.10/site-packages (from Deprecated->fiftyone) (1.17.2)\n",
      "Requirement already satisfied: wcwidth in ./health_care/lib/python3.10/site-packages (from ftfy->fiftyone) (0.2.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./health_care/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./health_care/lib/python3.10/site-packages (from matplotlib->fiftyone) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./health_care/lib/python3.10/site-packages (from matplotlib->fiftyone) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./health_care/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./health_care/lib/python3.10/site-packages (from matplotlib->fiftyone) (3.2.3)\n",
      "Requirement already satisfied: texttable in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.20.0 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (3.23.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.1.0)\n",
      "Requirement already satisfied: pyzstd>=0.16.1 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (0.17.0)\n",
      "Requirement already satisfied: pyppmd<1.3.0,>=1.1.0 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.2.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.6)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in ./health_care/lib/python3.10/site-packages (from py7zr->voxel51-eta<0.15,>=0.14.0->fiftyone) (1.0.3)\n",
      "Requirement already satisfied: networkx>=3.0 in ./health_care/lib/python3.10/site-packages (from scikit-image->fiftyone) (3.4.2)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./health_care/lib/python3.10/site-packages (from scikit-image->fiftyone) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./health_care/lib/python3.10/site-packages (from scikit-image->fiftyone) (2025.5.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./health_care/lib/python3.10/site-packages (from scikit-image->fiftyone) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./health_care/lib/python3.10/site-packages (from scikit-learn->fiftyone) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./health_care/lib/python3.10/site-packages (from scikit-learn->fiftyone) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets fiftyone pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f2347",
   "metadata": {},
   "source": [
    "### 🗂️ Loading and Merging ARCADE Dataset Subsets into FiftyOne\n",
    "\n",
    "The ARCADE challenge includes multiple datasets across two key tasks — **segmentation** and **stenosis detection** — spread across training, validation, and test phases. This section consolidates all subsets into a single FiftyOne dataset to simplify exploration and analysis.\n",
    "\n",
    "#### ✅ What this block does:\n",
    "- **Defines paths** to the original dataset structure.\n",
    "- **Lists all dataset splits**, categorized by task and phase (e.g., `seg_train`, `sten_val`, etc.).\n",
    "- **Deletes any previous dataset instance** named `\"arcade_combined\"` to ensure a clean workspace.\n",
    "- Iterates over each subset to:\n",
    "  - Locate image and annotation folders\n",
    "  - Load annotations in **COCO segmentation format**\n",
    "  - Create a temporary dataset using `COCODetectionDatasetImporter`\n",
    "  - Apply tags like `phase`, `task`, and `subset_name` to each sample\n",
    "  - Merge the tagged samples into a central `arcade_combined` dataset\n",
    "- **Cleans up** by deleting the temporary dataset after merging\n",
    "\n",
    "This creates a unified dataset in FiftyOne where you can filter by phase, task type, or subset name, setting the foundation for structured medical image analysis and model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc56fae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paularamos/Documents/FiftyOne_HealthCare_Workshop/health_care/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Importing seg_train...\n",
      " 100% |███████████████| 1000/1000 [3.6s elapsed, 0s remaining, 273.9 samples/s]      \n",
      "\n",
      "📦 Importing seg_val...\n",
      " 100% |█████████████████| 200/200 [778.0ms elapsed, 0s remaining, 257.1 samples/s]      \n",
      "\n",
      "📦 Importing sten_train...\n",
      " 100% |███████████████| 1000/1000 [1.4s elapsed, 0s remaining, 697.4 samples/s]         \n",
      "\n",
      "📦 Importing sten_val...\n",
      " 100% |█████████████████| 200/200 [324.7ms elapsed, 0s remaining, 617.6 samples/s]      \n",
      "\n",
      "📦 Importing test_case_seg...\n",
      " 100% |█████████████████| 300/300 [1.4s elapsed, 0s remaining, 220.8 samples/s]         \n",
      "\n",
      "📦 Importing test_case_sten...\n",
      " 100% |█████████████████| 300/300 [355.9ms elapsed, 0s remaining, 843.0 samples/s]     \n",
      "\n",
      "✅ All data successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "# Base directory where the dataset is located\n",
    "base_path = \"/Users/paularamos/Documents/datasets/arcade_challenge_datasets\"\n",
    "\n",
    "# Combined dataset name\n",
    "combined_dataset_name = \"arcade_combined\"\n",
    "\n",
    "# Dataset configuration\n",
    "datasets = [\n",
    "    (\"seg_train\", \"dataset_phase_1/segmentation_dataset/seg_train\", \"phase_1\", \"segmentation\"),\n",
    "    (\"seg_val\", \"dataset_phase_1/segmentation_dataset/seg_val\", \"phase_1\", \"segmentation\"),\n",
    "    (\"sten_train\", \"dataset_phase_1/stenosis_dataset/sten_train\", \"phase_1\", \"stenosis\"),\n",
    "    (\"sten_val\", \"dataset_phase_1/stenosis_dataset/sten_val\", \"phase_1\", \"stenosis\"),\n",
    "    (\"test_case_seg\", \"dataset_final_phase/test_case_segmentation\", \"final_phase\", \"segmentation\"),\n",
    "    (\"test_case_sten\", \"dataset_final_phase/test_cases_stenosis\", \"final_phase\", \"stenosis\"),\n",
    "]\n",
    "\n",
    "# Delete dataset if it already exists\n",
    "if fo.dataset_exists(combined_dataset_name):\n",
    "    fo.delete_dataset(combined_dataset_name)\n",
    "\n",
    "# Create empty combined dataset\n",
    "combined_dataset = fo.Dataset(name=combined_dataset_name)\n",
    "\n",
    "# Import each subset\n",
    "for subset_name, relative_path, phase, task in datasets:\n",
    "    print(f\"\\n📦 Importing {subset_name}...\")\n",
    "\n",
    "    image_dir = os.path.join(base_path, relative_path, \"images\")\n",
    "    annotation_dir = os.path.join(base_path, relative_path, \"annotations\")\n",
    "    json_files = [f for f in os.listdir(annotation_dir) if f.endswith(\".json\")]\n",
    "    assert len(json_files) == 1, f\"Expected 1 JSON file in {annotation_dir}, found {len(json_files)}\"\n",
    "    labels_path = os.path.join(annotation_dir, json_files[0])\n",
    "\n",
    "    # Temporary dataset\n",
    "    temp_dataset_name = f\"{combined_dataset_name}_{subset_name}\"\n",
    "    if fo.dataset_exists(temp_dataset_name):\n",
    "        fo.delete_dataset(temp_dataset_name)\n",
    "    temp_dataset = fo.Dataset(name=temp_dataset_name)\n",
    "\n",
    "    # Use COCO importer directly\n",
    "    importer = COCODetectionDatasetImporter(\n",
    "        data_path=image_dir,\n",
    "        labels_path=labels_path,\n",
    "        label_types=\"segmentations\",\n",
    "        include_id=True,\n",
    "        extra_attrs=True,\n",
    "    )\n",
    "    temp_dataset.add_importer(importer)\n",
    "\n",
    "    # Tag samples and move to combined dataset\n",
    "    for sample in temp_dataset:\n",
    "        sample.tags.extend([phase, task, subset_name])\n",
    "        combined_dataset.add_sample(sample)\n",
    "\n",
    "    temp_dataset.delete()  # cleanup\n",
    "\n",
    "# Launch app\n",
    "print(\"\\n✅ All data successfully loaded!\")\n",
    "#session = fo.launch_app(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eaf57d",
   "metadata": {},
   "source": [
    "### 🧬 Structured Import of ARCADE Subsets with Metadata Fields\n",
    "\n",
    "This cell loads and merges all six subsets of the ARCADE dataset into a unified FiftyOne dataset, while also enriching each sample with structured metadata fields. Compared to the previous version, this approach goes further by **explicitly adding schema fields** to enable advanced querying and filtering.\n",
    "\n",
    "#### 🔍 What's happening here:\n",
    "- Defines the paths and structure for the **segmentation** and **stenosis** datasets across training, validation, and final test phases.\n",
    "- Initializes a new dataset called `\"arcade_combined\"` and deletes any prior version with the same name.\n",
    "- **Adds custom fields** to the dataset schema:\n",
    "  - `phase` – e.g., `phase_1`, `final_phase`\n",
    "  - `task` – either `segmentation` or `stenosis`\n",
    "  - `subset_name` – specific subset like `seg_train` or `sten_val`\n",
    "- For each subset:\n",
    "  - Loads annotations in COCO format using `COCODetectionDatasetImporter`\n",
    "  - Creates a temporary dataset and populates it with images and labels\n",
    "  - Assigns field values and adds sample-level tags\n",
    "  - Merges the processed samples into the main dataset\n",
    "  - Deletes the temporary dataset after merging\n",
    "\n",
    "This structure allows for **highly flexible querying** (e.g., \"show all stenosis test cases\") and enables rich filtering and visualization workflows in FiftyOne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "from fiftyone.utils.coco import COCODetectionDatasetImporter\n",
    "\n",
    "# Base path\n",
    "base_path = \"/Users/paularamos/Documents/datasets/arcade_challenge_datasets\"\n",
    "combined_dataset_name = \"arcade_combined\"\n",
    "\n",
    "# Dataset config\n",
    "datasets = [\n",
    "    (\"seg_train\", \"dataset_phase_1/segmentation_dataset/seg_train\", \"phase_1\", \"segmentation\"),\n",
    "    (\"seg_val\", \"dataset_phase_1/segmentation_dataset/seg_val\", \"phase_1\", \"segmentation\"),\n",
    "    (\"sten_train\", \"dataset_phase_1/stenosis_dataset/sten_train\", \"phase_1\", \"stenosis\"),\n",
    "    (\"sten_val\", \"dataset_phase_1/stenosis_dataset/sten_val\", \"phase_1\", \"stenosis\"),\n",
    "    (\"test_case_seg\", \"dataset_final_phase/test_case_segmentation\", \"final_phase\", \"segmentation\"),\n",
    "    (\"test_case_sten\", \"dataset_final_phase/test_cases_stenosis\", \"final_phase\", \"stenosis\"),\n",
    "]\n",
    "\n",
    "# Delete existing dataset\n",
    "if fo.dataset_exists(combined_dataset_name):\n",
    "    fo.delete_dataset(combined_dataset_name)\n",
    "combined_dataset = fo.Dataset(combined_dataset_name)\n",
    "\n",
    "# Add metadata fields\n",
    "combined_dataset.add_sample_field(\"phase\", fo.StringField)\n",
    "combined_dataset.add_sample_field(\"task\", fo.StringField)\n",
    "combined_dataset.add_sample_field(\"subset_name\", fo.StringField)\n",
    "\n",
    "# Load each dataset separately and assign fields\n",
    "for subset_name, relative_path, phase, task in datasets:\n",
    "    print(f\"\\n📦 Loading {subset_name}...\")\n",
    "\n",
    "    image_dir = os.path.join(base_path, relative_path, \"images\")\n",
    "    annotation_dir = os.path.join(base_path, relative_path, \"annotations\")\n",
    "    json_files = [f for f in os.listdir(annotation_dir) if f.endswith(\".json\")]\n",
    "    assert len(json_files) == 1, f\"Expected 1 JSON in {annotation_dir}, found {len(json_files)}\"\n",
    "    labels_path = os.path.join(annotation_dir, json_files[0])\n",
    "\n",
    "    # Create temporary dataset\n",
    "    temp_dataset_name = f\"{combined_dataset_name}_{subset_name}\"\n",
    "    if fo.dataset_exists(temp_dataset_name):\n",
    "        fo.delete_dataset(temp_dataset_name)\n",
    "    temp_dataset = fo.Dataset(temp_dataset_name)\n",
    "\n",
    "    importer = COCODetectionDatasetImporter(\n",
    "        data_path=image_dir,\n",
    "        labels_path=labels_path,\n",
    "        label_types=\"segmentations\",\n",
    "        \n",
    "        include_id=True,\n",
    "        extra_attrs=True,\n",
    "    )\n",
    "\n",
    "    # Add to temp dataset\n",
    "    temp_dataset.add_importer(importer)\n",
    "\n",
    "    # Tag + assign fields\n",
    "    for sample in temp_dataset:\n",
    "        sample[\"phase\"] = phase\n",
    "        sample[\"task\"] = task\n",
    "        sample[\"subset_name\"] = subset_name\n",
    "        sample.tags.extend([subset_name, task, phase])\n",
    "        sample.save()\n",
    "        combined_dataset.add_sample(sample)\n",
    "\n",
    "    # Delete temp dataset\n",
    "    temp_dataset.delete()\n",
    "\n",
    "print(\"\\n✅ All subsets imported successfully!\")\n",
    "\n",
    "# Optional: launch FiftyOne app\n",
    "# fo.launch_app(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156f39d",
   "metadata": {},
   "source": [
    "### 🧠 Computing and Visualizing Embeddings with FiftyOne Brain\n",
    "\n",
    "Once the full ARCADE dataset has been assembled, this section applies **embedding generation and visualization** techniques using FiftyOne Brain. Embeddings are vector representations of images or patches that capture visual similarity and semantic content, enabling advanced tasks like clustering, outlier detection, and semantic search.\n",
    "\n",
    "#### 🗂 Step 1: Persist and Launch the App\n",
    "- We make the dataset **persistent** so it can be reloaded later.\n",
    "- The FiftyOne App is launched for interactive exploration of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "combined_dataset.persistent=True\n",
    "session = fo.launch_app(combined_dataset, port=5153, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628dce65",
   "metadata": {},
   "source": [
    "#### 🧠 Step 2: Compute Embeddings for Entire Images\n",
    "- Uses FiftyOne Brain’s `compute_visualization()` to generate **default embeddings** for each image.\n",
    "- These are stored under the `\"default_embedding\"` key and indexed using the `\"arcade_emb\"` brain key.\n",
    "- This allows for visualizing the full dataset in **2D latent space** (e.g., t-SNE or UMAP projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      " 100% |███████████████| 3000/3000 [5.2m elapsed, 0s remaining, 9.3 samples/s]      \n",
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paularamos/Documents/FiftyOne_HealthCare_Workshop/health_care/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP( verbose=True)\n",
      "Tue Jun 24 09:58:11 2025 Construct fuzzy simplicial set\n",
      "Tue Jun 24 09:58:14 2025 Finding Nearest Neighbors\n",
      "Tue Jun 24 09:58:16 2025 Finished Nearest Neighbor Search\n",
      "Tue Jun 24 09:58:17 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  33%| ███▎       163/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  45%| ████▌      227/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  66%| ██████▌    329/500 [00:01]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 500/500 [00:01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  450  /  500 epochs\n",
      "Tue Jun 24 09:58:19 2025 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<fiftyone.brain.visualization.VisualizationResults at 0x370b58a60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load dataset\n",
    "#dataset = fo.load_dataset(\"deeplesion\")\n",
    "\n",
    "# Compute embeddings for all samples\n",
    "fob.compute_visualization(\n",
    "    combined_dataset,\n",
    "    embeddings=\"default_embedding\",\n",
    "    brain_key=\"arcade_emb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71718b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        arcade_combined\n",
      "Media type:  image\n",
      "Num samples: 3000\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:                fiftyone.core.fields.ObjectIdField\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:        fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at:  fiftyone.core.fields.DateTimeField\n",
      "    phase:             fiftyone.core.fields.StringField\n",
      "    task:              fiftyone.core.fields.StringField\n",
      "    subset_name:       fiftyone.core.fields.StringField\n",
      "    segmentations:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    coco_id:           fiftyone.core.fields.IntField\n",
      "    default_embedding: fiftyone.core.fields.VectorField\n"
     ]
    }
   ],
   "source": [
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4759ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset.persistent=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac60f4",
   "metadata": {},
   "source": [
    "#### 🧩 Step 3: Compute Patch-Level Embeddings (Optional)\n",
    "- Instead of entire images, we compute embeddings for **regions of interest**, such as segmentations.\n",
    "- Specifies the patch field (`\"segmentations\"`) to extract ROIs.\n",
    "- Loads a pretrained model (`mobilenet-v2-imagenet-torch`) from the FiftyOne Model Zoo.\n",
    "- Computes embeddings for each patch and stores them under `\"patches_embedding\"`, with a new brain key `\"patches_emb\"`.\n",
    "\n",
    "These visual embeddings enable deep visual exploration and analysis of ARCADE’s segmentation and stenosis data, surfacing patterns and anomalies that go beyond raw pixel inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing patch embeddings...\n",
      " 100% |███████████████| 3000/3000 [13.1m elapsed, 0s remaining, 12.4 samples/s]      \n",
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Tue Jun 24 10:21:49 2025 Construct fuzzy simplicial set\n",
      "Tue Jun 24 10:21:49 2025 Finding Nearest Neighbors\n",
      "Tue Jun 24 10:21:49 2025 Building RP forest with 10 trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paularamos/Documents/FiftyOne_HealthCare_Workshop/health_care/lib/python3.10/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 24 10:21:52 2025 NN descent for 13 iterations\n",
      "\t 1  /  13\n",
      "\t 2  /  13\n",
      "\t 3  /  13\n",
      "\t 4  /  13\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Tue Jun 24 10:21:56 2025 Finished Nearest Neighbor Search\n",
      "Tue Jun 24 10:21:56 2025 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  22%| ██▏        43/200 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  200 epochs\n",
      "\tcompleted  20  /  200 epochs\n",
      "\tcompleted  40  /  200 epochs\n",
      "\tcompleted  60  /  200 epochs\n",
      "\tcompleted  80  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed:  84%| ████████▍  169/200 [00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  100  /  200 epochs\n",
      "\tcompleted  120  /  200 epochs\n",
      "\tcompleted  140  /  200 epochs\n",
      "\tcompleted  160  /  200 epochs\n",
      "\tcompleted  180  /  200 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 24 10:21:56 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# Specify the field containing the patches (e.g., detections)\n",
    "patches_field = \"segmentations\"\n",
    "\n",
    "# Option 1: Use a pre-trained model from the FiftyOne Model Zoo\n",
    "model = foz.load_zoo_model(\"mobilenet-v2-imagenet-torch\")\n",
    "\n",
    "# Compute embeddings for the patches using the specified model\n",
    "results = fob.compute_visualization(\n",
    "    combined_dataset, \n",
    "    patches_field=patches_field, \n",
    "    model=model, \n",
    "    embeddings=\"patches_embedding\", \n",
    "    brain_key=\"patches_emb\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c2890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        arcade_combined\n",
      "Media type:  image\n",
      "Num samples: 3000\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:                fiftyone.core.fields.ObjectIdField\n",
      "    filepath:          fiftyone.core.fields.StringField\n",
      "    tags:              fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:        fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at:  fiftyone.core.fields.DateTimeField\n",
      "    phase:             fiftyone.core.fields.StringField\n",
      "    task:              fiftyone.core.fields.StringField\n",
      "    subset_name:       fiftyone.core.fields.StringField\n",
      "    segmentations:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "    coco_id:           fiftyone.core.fields.IntField\n",
      "    default_embedding: fiftyone.core.fields.VectorField\n"
     ]
    }
   ],
   "source": [
    "combined_dataset.reload()\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e191c",
   "metadata": {},
   "source": [
    "### 🗄️ Exporting the Combined ARCADE Dataset and Publishing to Hugging Face 🤗\n",
    "\n",
    "This final step packages the merged ARCADE dataset into a shareable format and uploads it to the Hugging Face Hub.\n",
    "\n",
    "#### 🚚 Step 1: Clone the Dataset\n",
    "We create a **clone** of the `arcade_combined` dataset to keep the export operation clean and isolated. This cloned dataset is named `arcade_combined_export`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999433db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the dataset\n",
    "cloned_dataset = combined_dataset.clone(name=\"arcade_combined_export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee64010",
   "metadata": {},
   "source": [
    "#### 💾 Step 2: Export to Local Directory\n",
    "Using `FiftyOneDataset` format, the cloned dataset is exported to a directory (`./arcade_combined_fiftyone`). This includes:\n",
    "- Image files\n",
    "- COCO-style annotations (in this case, from the `\"segmentations\"` field)\n",
    "- Dataset metadata\n",
    "\n",
    "The `overwrite=True` flag ensures any existing content in the folder is safely replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b11d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting samples...\n",
      " 100% |██████████████████| 3000/3000 [3.1s elapsed, 0s remaining, 1.1K docs/s]       \n"
     ]
    }
   ],
   "source": [
    "# Set export directory\n",
    "export_dir = \"./arcade_combined_fiftyone\"\n",
    "\n",
    "# Export in FiftyOne format\n",
    "cloned_dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    label_field=\"segmentations\",  # or other label field you want to export\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72e57c",
   "metadata": {},
   "source": [
    "#### 🤗 Step 3: Push to Hugging Face Hub\n",
    "With just one line of code, we use `push_to_hub()` to upload the dataset directly to the Hugging Face Hub. The dataset will appear under the user or organization space defined by the credentials used.\n",
    "\n",
    "Publishing to Hugging Face allows others to:\n",
    "- **Easily reuse your dataset** in notebooks, apps, or models\n",
    "- **View it through the FiftyOne Web App integration**\n",
    "- **Promote reproducibility and collaboration** in open-source healthcare AI research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee2d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/var/folders/6y/g2mslh_s7fz7qtj9vrxntqtm0000gn/T/tmp87ezk81j' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |██████████████████| 3000/3000 [3.0s elapsed, 0s remaining, 1.1K docs/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading media files in 2 batches of size 3000:   0%|          | 0/2 [00:00<?, ?it/s]It seems you are trying to upload a large folder at once. This might take some time and then fail if the folder is too large. For such cases, it is recommended to upload in smaller batches or to use `HfApi().upload_large_folder(...)`/`huggingface-cli upload-large-folder` instead. For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\n",
      "Uploading media files in 2 batches of size 3000:  50%|█████     | 1/2 [02:48<02:48, 168.37s/it]No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "Uploading media files in 2 batches of size 3000: 100%|██████████| 2/2 [02:49<00:00, 84.63s/it] \n"
     ]
    }
   ],
   "source": [
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "\n",
    "push_to_hub(cloned_dataset, \"arcade_fiftyone\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
