{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478307bd",
   "metadata": {},
   "source": [
    "# ðŸ¥ Visual AI in Healthcare with FiftyOne - MedSAM2 on CT Scans  \n",
    "**Empowering medical imaging workflows with open-source tools and modern AI**\n",
    "\n",
    "This notebook is part of the **â€œVisual AI in Healthcare with FiftyOneâ€** workshop. Here, we explore how to **load and prepare CT video data with MedSAM2 annotations** from Hugging Face into FiftyOne for visual AI workflows in healthcare.\n",
    "\n",
    "ðŸ”¬ **What youâ€™ll learn in this notebook:**\n",
    "\n",
    "- How to **load the BTCV-CT-as-video-MedSAM2 dataset** from Hugging Face using FiftyOne utilities  \n",
    "- How to **preview CT scans as video sequences** within the FiftyOne App  \n",
    "- How to **inspect and filter frame-level detections** to isolate relevant slices   \n",
    "\n",
    "ðŸ“š **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` â€“ Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` â€“ Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` â€“ Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` â€“ Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` â€“ Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` â€“ Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` â€“ Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ee1e2e",
   "metadata": {},
   "source": [
    "### âœ… Requirements\n",
    "\n",
    "Please install all the requeriments for running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf035127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets fiftyone pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af8590",
   "metadata": {},
   "source": [
    "Clean up previous dataset (if needed)\n",
    "\n",
    "Before loading the dataset, we ensure there's no existing dataset with the same name in the FiftyOne environment.\n",
    "\n",
    "- The dataset we're working with is the **BTCV-CT-as-video-MedSAM2**, hosted on Hugging Face and loaded via FiftyOne.\n",
    "- FiftyOne stores datasets locally, so we check if one with the same name exists and delete it to avoid conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Name used internally by FiftyOne (it does not use slashes like 'Voxel51/BTCV-...')\n",
    "dataset_name = \"Voxel51/BTCV-CT-as-video-MedSAM2-dataset\"\n",
    "\n",
    "# Delete the dataset if it exists\n",
    "if fo.dataset_exists(dataset_name):\n",
    "    fo.delete_dataset(dataset_name)\n",
    "    print(f\"Deleted existing dataset: {dataset_name}\")\n",
    "else:\n",
    "    print(f\"No dataset found with name: {dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc78ba0d",
   "metadata": {},
   "source": [
    "### Load BTCV CT Scan Video Dataset from Hugging Face\n",
    "\n",
    "In this step, we load a sample from the **BTCV-CT-as-video-MedSAM2** dataset using FiftyOneâ€™s integration with Hugging Face.\n",
    "\n",
    "- We import the dataset using `load_from_hub()` and keep only the first 2 samples to simplify our analysis.\n",
    "- This dataset is structured as **videos of CT scans** with **MedSAM2-predicted segmentations**.\n",
    "- We then **filter out all frames except the middle one (frame 100)** to focus on a single annotated frame for each video.\n",
    "- This is necessary because MedSAM2 only propagates masks forward in video, and this step helps us simplify the visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc56fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "\n",
    "dataset = load_from_hub(\"Voxel51/BTCV-CT-as-video-MedSAM2-dataset\")[:2]\n",
    "\n",
    "\n",
    "# Retaining detections from a single frame in the middle\n",
    "# Note that SAM2 only propagates segmentation masks forward in a video\n",
    "(\n",
    "    dataset\n",
    "    .match_frames(F(\"frame_number\") != 100)\n",
    "    .set_field(\"frames.gt_detections\", None)\n",
    "    .save()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dcc6a6",
   "metadata": {},
   "source": [
    "### ðŸ› ï¸ Install and Validate `sam2` Package\n",
    "\n",
    "To run MedSAM2 locally or inspect its behavior, we clone and install the official [facebookresearch/sam2](https://github.com/facebookresearch/sam2) repository.\n",
    "\n",
    "However, an issue may arise during installation due to the version of `setuptools`. If you encounter a build error, modify the `pyproject.toml` file as follows:\n",
    "\n",
    "**Fix for `setuptools` version issue:**\n",
    "\n",
    "In the file `sam2/pyproject.toml`, update the dependency line:\n",
    "\n",
    "```toml\n",
    "# Before:\n",
    "setuptools>=61.0\n",
    "\n",
    "# After:\n",
    "setuptools>=62.3.0,<75.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dd98fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/facebookresearch/sam2.git && cd sam2\n",
    "#pip install -e .\n",
    "\n",
    "import sam2\n",
    "print(hasattr(sam2, 'build_sam'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f7b73",
   "metadata": {},
   "source": [
    "### ðŸ¤– Apply MedSAM2 Video Segmentation Model\n",
    "\n",
    "We now load the **MedSAM2 video model** from the FiftyOne Model Zoo and apply it to our dataset. This model supports segmenting anatomical structures across frames in a medical video.\n",
    "\n",
    "- **Model**: `med-sam-2-video-torch`  \n",
    "- **Prompt**: Uses `frames.gt_detections` (bounding boxes) as the prompt to generate masks  \n",
    "- **Output**: Stores results in `frames.pred_segmentations`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e167c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\"med-sam-2-video-torch\")\n",
    "\n",
    "# Segment inside boxes and propagate to all frames\n",
    "dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"pred_segmentations\",\n",
    "    prompt_field=\"frames.gt_detections\",\n",
    ")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
