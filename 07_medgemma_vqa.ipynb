{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Visual AI in Healthcare with FiftyOne â€“ VQA and Classification with MedGemma  \n",
    "**Harnessing MedGemmaâ€™s multimodal medical expertise for visual question answering and image classification**\n",
    "\n",
    "This notebook is part of the **â€œVisual AI in Healthcare with FiftyOneâ€** workshop. In this hands-on example, we use the **instruction-tuned MedGemma 4B** model (from Hugging Face) to run **visual question answering (VQA)** and **classification** tasks on a small medical dataset via FiftyOneâ€™s Hugging Face integration.\n",
    "\n",
    "ðŸ”¬ **What youâ€™ll learn in this notebook:**\n",
    "\n",
    "- How to **load the MedXpertQA dataset** from Hugging Face using `fiftyone.utils.huggingface`  \n",
    "- How to **run MedGemma (4B-it) locally** with a custom system prompt and input image  \n",
    "- How to **perform VQA and classification** using the `image-text-to-text` pipeline  \n",
    "- How to **store MedGemmaâ€™s output** back into your FiftyOne samples for analysis  \n",
    "\n",
    "âš™ï¸ **MedGemma quick facts**:\n",
    "- Developed by **Google**, MedGemma is built on Gemma 3 with a **SigLIP image encoder**\n",
    "- Trained on diverse medical images and text (CXR, dermoscopy, histopathology, ophthalmology)\n",
    "- Supports **instruction tuning** for healthcare applications like report generation, diagnosis support, and multimodal Q&A  \n",
    "- Available on [Hugging Face](https://huggingface.co/google/medgemma-4b-it) and via [Google Model Garden](https://cloud.google.com/model-garden)  \n",
    "\n",
    "ðŸ“š **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` â€“ Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` â€“ Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` â€“ Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` â€“ Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` â€“ Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` â€“ Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` â€“ Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Requirements\n",
    "\n",
    "Before running this notebook, make sure you have the required libraries installed.\n",
    "\n",
    "You can install them using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub>=0.20.0 torch torchvision transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Load the MedXpertQA Dataset from Hugging Face\n",
    "\n",
    "We start by loading a subset of the **MedXpertQA** dataset â€” a benchmark designed to evaluate expert-level reasoning in medical visual question answering.  \n",
    "This dataset includes a combination of images and textual medical questions across various specialties.\n",
    "\n",
    "We use the `load_from_hub()` utility from `fiftyone.utils.huggingface` to fetch the dataset directly from Hugging Face and import it into FiftyOne.\n",
    "\n",
    "ðŸ”¹ **Key actions in this step:**\n",
    "- Load 10 multimodal samples for quick experimentation\n",
    "- Assign a name `MedXpertQA` to the dataset for reference in the FiftyOne App\n",
    "- Set `overwrite=True` to reset the dataset if it already exists locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/MedXpertQA\",\n",
    "    name=\"MedXpertQA\",\n",
    "    max_samples=10,\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and Load MedGemma from a Custom Zoo Source - Setup Zoo Model \n",
    "\n",
    "FiftyOne allows you to register custom model sources and use them just like built-in zoo models.\n",
    "\n",
    "In this step, we:\n",
    "- Register a **custom Zoo model source** pointing to a GitHub repo that defines the MedGemma integration\n",
    "- Download the **instruction-tuned MedGemma 4B** model (`https://github.com/harpreetsahota204/medgemm`)\n",
    "- Load the model using FiftyOneâ€™s `load_zoo_model()` interface with optional quantization for efficient inference\n",
    "\n",
    "ðŸ”¹ **Why use a custom zoo?**  \n",
    "It lets us easily manage models that are not part of the default FiftyOne model zoo but are useful for specific domains â€” like MedGemma for healthcare.\n",
    "\n",
    "> If running this for the first time, you may need to uncomment `install_requirements=True` to automatically install dependencies listed by the model repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/medgemma\", overwrite=True)\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma\",\n",
    "    model_name=\"google/medgemma-4b-it\", \n",
    ")\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    quantized=True,\n",
    "    # install_requirements=True #run this to install requirements if they're not already\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ·ï¸ Run Body System Classification with MedGemma\n",
    "\n",
    "You can use this model for a zero-shot classification task as follows, which will add a [FiftyOne Classificaton](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) to your dataset. We can apply it to classify each image in the dataset based on its **body system**.\n",
    "\n",
    "ðŸ”¹ **Whatâ€™s happening here:**\n",
    "- We extract the **distinct body system labels** from the dataset (`body_system.label`)\n",
    "- We configure the modelâ€™s operation to `\"classify\"`\n",
    "- We define a **custom system prompt** that guides MedGemma to classify images into **one of the known body systems**\n",
    "- Finally, we run `apply_model()` to store MedGemmaâ€™s predictions in a new field called `\"pred_body_system\"`\n",
    "\n",
    "This showcases MedGemma's zero-shot classification capabilities when paired with structured prompts and small medical datasets.\n",
    "\n",
    "> You can inspect predictions later in the FiftyOne App and compare them with ground truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_system_labels = dataset.distinct(\"body_system.label\")\n",
    "\n",
    "model.operation = \"classify\"\n",
    "\n",
    "model.prompt = \"As a medical expert your task is to classify this image into exactly one of the following body systems: \" + \", \".join(body_system_labels)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"pred_body_system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as follows to guide the model's response. Note that we are using an existing field on the sample by passing `prompt_field=\"question\"` into the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html) method of the [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"classify\"\n",
    "\n",
    "model.system_prompt = \"\"\"You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
    "\n",
    "You are presented with the medical history of a patient and an accompanying image related to the patient. \n",
    "\n",
    "You will be asked a multiple question and are required to select from one of the available answers.\n",
    "\n",
    "Once you have the answer, you  must respond in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your single letter answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks and respond only with a one letter answer (one of A, B, C, D, E).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"model_answer\", \n",
    "    prompt_field=\"question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MedGemma for VQA\n",
    "\n",
    "You can use the model for visual question answering as shown below. This example will use the same prompt on each [Sample](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "model.prompt = \"Describe the condition the image you see here\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the default system prompt, which you can inspect as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as shown above, by simply following this pattern:\n",
    "\n",
    "```python\n",
    "model.system_prompt = \"An awesome customized system prompt\"\n",
    "```\n",
    "\n",
    "You can use a [Field](https://docs.voxel51.com/api/fiftyone.core.fields.html#module-fiftyone.core.fields) from the sample as a prompt by passing in `prompt_field=<field-name>` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"verbose_ans\", prompt_field=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne App with the filtered dataset\n",
    "session = fo.launch_app(dataset, port=5151, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.persistent=True\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
