{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Visual AI in Healthcare with FiftyOne â€“ VQA and Classification with MedGemma  \n",
    "**Harnessing MedGemmaâ€™s multimodal medical expertise for visual question answering and image classification**\n",
    "\n",
    "This notebook is part of the **â€œVisual AI in Healthcare with FiftyOneâ€** workshop. In this hands-on example, we use the **instruction-tuned MedGemma 4B** model (from Hugging Face) to run **visual question answering (VQA)** and **classification** tasks on a small medical dataset via FiftyOneâ€™s Hugging Face integration.\n",
    "\n",
    "ðŸ”¬ **What youâ€™ll learn in this notebook:**\n",
    "\n",
    "- How to **load the MedXpertQA dataset** from Hugging Face using `fiftyone.utils.huggingface`  \n",
    "- How to **run MedGemma (4B-it) locally** with a custom system prompt and input image  \n",
    "- How to **perform VQA and classification** using the `image-text-to-text` pipeline  \n",
    "- How to **store MedGemmaâ€™s output** back into your FiftyOne samples for analysis  \n",
    "\n",
    "âš™ï¸ **MedGemma quick facts**:\n",
    "- Developed by **Google**, MedGemma is built on Gemma 3 with a **SigLIP image encoder**\n",
    "- Trained on diverse medical images and text (CXR, dermoscopy, histopathology, ophthalmology)\n",
    "- Supports **instruction tuning** for healthcare applications like report generation, diagnosis support, and multimodal Q&A  \n",
    "- Available on [Hugging Face](https://huggingface.co/google/medgemma-4b-it) and via [Google Model Garden](https://cloud.google.com/model-garden)  \n",
    "\n",
    "ðŸ“š **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` â€“ Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` â€“ Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` â€“ Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` â€“ Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` â€“ Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` â€“ Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` â€“ Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Requirements\n",
    "\n",
    "Before running this notebook, make sure you have the required libraries installed.\n",
    "\n",
    "You can install them using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub>=0.20.0 torch torchvision transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“¦ Load the SLAKE Dataset from Hugging Face\n",
    "\n",
    "We start by loading a subset of the **SLAKE** dataset â€” a benchmark designed to evaluate expert-level reasoning in medical visual question answering.  \n",
    "This dataset includes a combination of images and textual medical questions across various specialties.\n",
    "\n",
    "We use the `load_from_hub()` utility from `fiftyone.utils.huggingface` to fetch the dataset directly from Hugging Face and import it into FiftyOne.\n",
    "\n",
    "ðŸ”¹ **Key actions in this step:**\n",
    "- Load 10 multimodal samples for quick experimentation\n",
    "- Assign a name `SLAKE` to the dataset for reference in the FiftyOne App\n",
    "- Set `overwrite=True` to reset the dataset if it already exists locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/SLAKE\",\n",
    "    name=\"SLAKE\",\n",
    "    overwrite=True,\n",
    "    max_samples=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and Load MedGemma from a Custom Zoo Source - Setup Zoo Model \n",
    "\n",
    "FiftyOne allows you to register custom model sources and use them just like built-in zoo models.\n",
    "\n",
    "In this step, we:\n",
    "- Register a **custom Zoo model source** pointing to a GitHub repo that defines the MedGemma integration\n",
    "- Download the **instruction-tuned MedGemma 4B** model (`https://github.com/harpreetsahota204/medgemm`)\n",
    "- Load the model using FiftyOneâ€™s `load_zoo_model()` interface with optional quantization for efficient inference\n",
    "\n",
    "ðŸ”¹ **Why use a custom zoo?**  \n",
    "It lets us easily manage models that are not part of the default FiftyOne model zoo but are useful for specific domains â€” like MedGemma for healthcare.\n",
    "\n",
    "> If running this for the first time, you may need to uncomment `install_requirements=True` to automatically install dependencies listed by the model repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/medgemma\", overwrite=True)\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma\",\n",
    "    model_name=\"google/medgemma-4b-it\", \n",
    ")\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    quantized=True,\n",
    "    # install_requirements=True #run this to install requirements if they're not already\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ·ï¸ Run Body System Classification with MedGemma\n",
    "\n",
    "You can use this model for a zero-shot classification task as follows, which will add a [FiftyOne Classificaton](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) to your dataset. We can apply it to classify each image in the dataset based on its **body system**.\n",
    "\n",
    "ðŸ”¹ **Whatâ€™s happening here:**\n",
    "- We extract the **distinct body system labels** from the dataset (`body_system.label`)\n",
    "- We configure the modelâ€™s operation to `\"classify\"`\n",
    "- We define a **custom system prompt** that guides MedGemma to classify images into **one of the known body systems**\n",
    "- Finally, we run `apply_model()` to store MedGemmaâ€™s predictions in a new field called `\"pred_body_system\"`\n",
    "\n",
    "This showcases MedGemma's zero-shot classification capabilities when paired with structured prompts and small medical datasets.\n",
    "\n",
    "> You can inspect predictions later in the FiftyOne App and compare them with ground truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_system_labels = dataset.distinct(\"modality.label\")\n",
    "\n",
    "model.operation = \"classify\"\n",
    "\n",
    "model.prompt = \"As a medical expert your task is to classify this image into exactly one of the following types: \" + \", \".join(body_system_labels)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"pred_modality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as follows to guide the model's response. Note that we are using an existing field on the sample by passing `prompt_field=\"question\"` into the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html) method of the [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()['modality']\n",
    "dataset.first()['pred_modality.classifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MedGemma for VQA\n",
    "\n",
    "You can use the model for visual question answering as shown below. This example will use the same prompt on each [Sample](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) in the Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the default system prompt, which you can inspect as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as follows to guide the model's response. Note that we are using an existing field on the sample by passing `prompt_field=\"question\"` into the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html) method of the [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html).\n",
    "\n",
    "Note that if you want to parse the model output as a [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) then you need to very specifically prompt the model to output in a way that this integration expects, that is:\n",
    "\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Notice below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.operation=\"classify\"\n",
    "\n",
    "model.system_prompt = \"\"\"You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
    "\n",
    "You will be asked a question and are required to provide your answer. Your answer must be in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks and respond only with one answer.\n",
    "\"\"\"\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"pred_answer_6\", \n",
    "    prompt_field=\"question_6\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which organ is abnormal, heart or lung?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.first()['question_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Classification: {\n",
       "    'id': '6830a40a7a3316c437168c0e',\n",
       "    'tags': [],\n",
       "    'label': 'Heart',\n",
       "    'confidence': None,\n",
       "    'logits': None,\n",
       "}>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.first()['answer_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Classification: {\n",
       "     'id': '6830d5a0d69a8c1f151f7bc2',\n",
       "     'tags': [],\n",
       "     'label': 'Heart',\n",
       "     'confidence': None,\n",
       "     'logits': None,\n",
       " }>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.first()['pred_answer_6.classifications']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For open-ended generation, you can use `vqa` mode. Note that with both modes you can use a single question on each sample as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.system_prompt = None # we need to clear the custom system prompt  \n",
    "\n",
    "model.operation=\"vqa\"\n",
    "\n",
    "model.prompt = \"Describe any anomolies in this the image that you observe.\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"open_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I will analyze the chest X-ray provided.\n",
      "\n",
      "**Observations:**\n",
      "\n",
      "*   **Cardiomegaly:** The heart appears enlarged (cardiomegaly). This is evident by the increased prominence of the cardiac silhouette, particularly the left ventricle.\n",
      "*   **Mediastinal Devices:** There are multiple devices visible within the mediastinum, including a pacemaker/ICD device. The leads appear to be in standard positions.\n",
      "*   **Pulmonary Vascularity:** The pulmonary vasculature appears relatively normal in terms of distribution and prominence.\n",
      "*   **Lung Fields:** The lung fields appear clear with no obvious consolidation, effusions, or masses.\n",
      "*   **Bones:** The bony structures of the ribs and sternum appear intact.\n",
      "*   **Mediastinal Width:** The mediastinal width appears to be within normal limits.\n",
      "\n",
      "**Potential Anomalies/Considerations:**\n",
      "\n",
      "*   **Cardiomegaly:** The cardiomegaly could be due to various factors, including hypertension, valvular heart disease, cardiomyopathy, or congenital heart defects. Further evaluation with echocardiography is needed to determine the underlying cause.\n",
      "*   **Device Placement:** The pacemaker/ICD device appears to be in standard position. However, the specific type of device and the presence of any complications (e.g., lead dislodgement, infection) would require further evaluation.\n",
      "\n",
      "**Disclaimer:** This is a preliminary interpretation based on a single chest X-ray. A definitive diagnosis requires a comprehensive clinical evaluation, including patient history, physical examination, and potentially additional imaging studies (e.g., echocardiography, CT scan).\n",
      "\n",
      "**In summary, the chest X-ray shows cardiomegaly and the presence of mediastinal devices. Further evaluation is needed to determine the underlying cause of the cardiomegaly and to assess the status of the mediastinal devices.**\n",
      "\n",
      "**Important Note:** This analysis is for informational purposes only and should not be considered a substitute for professional medical advice.\n"
     ]
    }
   ],
   "source": [
    "print(dataset.first()[\"open_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use you can use a Sample field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------------|  0/50 [3.2ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2% |/------------------|  1/50 [6.7s elapsed, 5.5m remaining, 0.1 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4% |-------------------|  2/50 [8.7s elapsed, 3.5m remaining, 0.2 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6% |â–ˆ\\-----------------|  3/50 [10.7s elapsed, 2.8m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8% |â–ˆ|-----------------|  4/50 [12.0s elapsed, 2.3m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  10% |â–ˆ/-----------------|  5/50 [17.5s elapsed, 2.6m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12% |â–ˆâ–ˆ-----------------|  6/50 [19.2s elapsed, 2.3m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14% |â–ˆâ–ˆ\\----------------|  7/50 [20.7s elapsed, 2.1m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16% |â–ˆâ–ˆâ–ˆ|---------------|  8/50 [23.3s elapsed, 2.0m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18% |â–ˆâ–ˆâ–ˆ/---------------|  9/50 [32.6s elapsed, 2.5m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20% |â–ˆâ–ˆâ–ˆ----------------| 10/50 [33.9s elapsed, 2.2m remaining, 0.3 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"answer_5_vqa\", \n",
    "    prompt_field=\"question_5\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"question_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"answer_5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.first()[\"answer_5_vqa\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "\n",
    "You can use FiftyOne's [Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) to evaluate model performance via the SDK. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dataset.evaluate_classifications(\n",
    "    \"pred_modality\",\n",
    "    gt_field=\"modality\",\n",
    "    eval_key=\"eval_simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then review as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = results.plot_confusion_matrix()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can, of course, do all of this in the App as shown here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
