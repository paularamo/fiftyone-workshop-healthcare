{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Visual AI in Healthcare with FiftyOne – VQA and Classification with MedGemma  \n",
    "**Harnessing MedGemma’s multimodal medical expertise for visual question answering and image classification**\n",
    "\n",
    "This notebook is part of the **“Visual AI in Healthcare with FiftyOne”** workshop. In this hands-on example, we use the **instruction-tuned MedGemma 4B** model (from Hugging Face) to run **visual question answering (VQA)** and **classification** tasks on a small medical dataset via FiftyOne’s Hugging Face integration.\n",
    "\n",
    "🔬 **What you’ll learn in this notebook:**\n",
    "\n",
    "- How to **load the MedXpertQA dataset** from Hugging Face using `fiftyone.utils.huggingface`  \n",
    "- How to **run MedGemma (4B-it) locally** with a custom system prompt and input image  \n",
    "- How to **perform VQA and classification** using the `image-text-to-text` pipeline  \n",
    "- How to **store MedGemma’s output** back into your FiftyOne samples for analysis  \n",
    "\n",
    "⚙️ **MedGemma quick facts**:\n",
    "- Developed by **Google**, MedGemma is built on Gemma 3 with a **SigLIP image encoder**\n",
    "- Trained on diverse medical images and text (CXR, dermoscopy, histopathology, ophthalmology)\n",
    "- Supports **instruction tuning** for healthcare applications like report generation, diagnosis support, and multimodal Q&A  \n",
    "- Available on [Hugging Face](https://huggingface.co/google/medgemma-4b-it) and via [Google Model Garden](https://cloud.google.com/model-garden)  \n",
    "\n",
    "📚 **Part of the notebook series:**\n",
    "1. `01_load_arcade_dataset.ipynb` – Load and visualize the ARCADE dataset.  \n",
    "2. `02_load_deeplesion_balanced.ipynb` – Curate and balance the DeepLesion dataset.  \n",
    "3. `03_vlms_analysis_arcade.ipynb` – Use VFMs like NVLabs_CRADIOV3 in dataset undersatnding for ARCADE. \n",
    "4. `04_finetune_yolo8_stenosis.ipynb` – Train and integrate YOLOv8 for stenosis detection.  \n",
    "5. `05_medsam2_ct_scan.ipynb` – Run MedSAM2 on CT scans for segmentation.  \n",
    "6. `06_nvidia_vista_segmentation.ipynb` – Explore NVIDIA-VISTA-3D.  \n",
    "7. `07_medgemma_vqa.ipynb` – Perform visual question answering and classification with MedGemma.\n",
    "\n",
    "All notebooks are standalone but are best experienced sequentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✅ Requirements\n",
    "\n",
    "Before running this notebook, make sure you have the required libraries installed.\n",
    "\n",
    "You can install them using `pip`:\n",
    "\n",
    "```bash\n",
    "pip install huggingface_hub>=0.20.0 torch torchvision transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Load the MedXpertQA Dataset from Hugging Face\n",
    "\n",
    "We start by loading a subset of the **MedXpertQA** dataset — a benchmark designed to evaluate expert-level reasoning in medical visual question answering.  \n",
    "This dataset includes a combination of images and textual medical questions across various specialties.\n",
    "\n",
    "We use the `load_from_hub()` utility from `fiftyone.utils.huggingface` to fetch the dataset directly from Hugging Face and import it into FiftyOne.\n",
    "\n",
    "🔹 **Key actions in this step:**\n",
    "- Load 10 multimodal samples for quick experimentation\n",
    "- Assign a name `MedXpertQA` to the dataset for reference in the FiftyOne App\n",
    "- Set `overwrite=True` to reset the dataset if it already exists locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paula/Documents/fiftyone_healthcare_workshop/FO_health_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from Voxel51/MedXpertQA\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |███████████████████| 10/10 [2.1ms elapsed, 0s remaining, 4.7K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/MedXpertQA\",\n",
    "    name=\"MedXpertQA\",\n",
    "    max_samples=10,\n",
    "    overwrite=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        MedXpertQA\n",
       "Media type:  image\n",
       "Num samples: 10\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    question_id:      fiftyone.core.fields.StringField\n",
       "    question:         fiftyone.core.fields.StringField\n",
       "    options:          fiftyone.core.fields.DictField\n",
       "    label:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    medical_task:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    body_system:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    question_type:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register and Load MedGemma from a Custom Zoo Source - Setup Zoo Model \n",
    "\n",
    "FiftyOne allows you to register custom model sources and use them just like built-in zoo models.\n",
    "\n",
    "In this step, we:\n",
    "- Register a **custom Zoo model source** pointing to a GitHub repo that defines the MedGemma integration\n",
    "- Download the **instruction-tuned MedGemma 4B** model (`https://github.com/harpreetsahota204/medgemm`)\n",
    "- Load the model using FiftyOne’s `load_zoo_model()` interface with optional quantization for efficient inference\n",
    "\n",
    "🔹 **Why use a custom zoo?**  \n",
    "It lets us easily manage models that are not part of the default FiftyOne model zoo but are useful for specific domains — like MedGemma for healthcare.\n",
    "\n",
    "> If running this for the first time, you may need to uncomment `install_requirements=True` to automatically install dependencies listed by the model repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/harpreetsahota204/medgemma...\n",
      "  185.4Kb [15.2ms elapsed, ? remaining, 11.9Mb/s] \n",
      "Overwriting existing model source '/home/paula/fiftyone/__models__/medgemma'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 15 files: 100%|██████████| 15/15 [00:02<00:00,  5.06it/s]\n",
      "Fetching 15 files: 100%|██████████| 15/15 [00:00<00:00, 1671.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MedGemma model from /home/paula/fiftyone/__models__/medgemma/medgemma-4b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "foz.register_zoo_model_source(\"https://github.com/harpreetsahota204/medgemma\", overwrite=True)\n",
    "\n",
    "foz.download_zoo_model(\n",
    "    \"https://github.com/harpreetsahota204/medgemma\",\n",
    "    model_name=\"google/medgemma-4b-it\", \n",
    ")\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"google/medgemma-4b-it\",\n",
    "    quantized=True,\n",
    "    # install_requirements=True #run this to install requirements if they're not already\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏷️ Run Body System Classification with MedGemma\n",
    "\n",
    "You can use this model for a zero-shot classification task as follows, which will add a [FiftyOne Classificaton](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) to your dataset. We can apply it to classify each image in the dataset based on its **body system**.\n",
    "\n",
    "🔹 **What’s happening here:**\n",
    "- We extract the **distinct body system labels** from the dataset (`body_system.label`)\n",
    "- We configure the model’s operation to `\"classify\"`\n",
    "- We define a **custom system prompt** that guides MedGemma to classify images into **one of the known body systems**\n",
    "- Finally, we run `apply_model()` to store MedGemma’s predictions in a new field called `\"pred_body_system\"`\n",
    "\n",
    "This showcases MedGemma's zero-shot classification capabilities when paired with structured prompts and small medical datasets.\n",
    "\n",
    "> You can inspect predictions later in the FiftyOne App and compare them with ground truth labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------------|  0/10 [1.8ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aeb                                            \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  10% |█/-----------------|  1/10 [253.5ms elapsed, 2.3s remaining, 3.9 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aec                                                   \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aed\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aee\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aef\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af0\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af1\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  70% |█████████████------|  7/10 [359.1ms elapsed, 153.9ms remaining, 19.5 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707af2                                                       \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af3\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af4\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      " 100% |███████████████████| 10/10 [432.4ms elapsed, 0s remaining, 23.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "body_system_labels = dataset.distinct(\"body_system.label\")\n",
    "\n",
    "model.operation = \"classify\"\n",
    "\n",
    "model.prompt = \"As a medical expert your task is to classify this image into exactly one of the following body systems: \" + \", \".join(body_system_labels)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"pred_body_system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as follows to guide the model's response. Note that we are using an existing field on the sample by passing `prompt_field=\"question\"` into the [`apply_model`](https://docs.voxel51.com/api/fiftyone.core.dataset.html) method of the [Dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------------|  0/10 [3.1ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aeb                                            \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aec\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aed\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aee\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  40% |███████/-----------|  4/10 [115.9ms elapsed, 173.8ms remaining, 34.5 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aef                                                       \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af0\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af1\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af2\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  80% |███████████████----|  8/10 [218.7ms elapsed, 54.7ms remaining, 36.6 samples/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707af3                                                       \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af4\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      " 100% |███████████████████| 10/10 [266.2ms elapsed, 0s remaining, 37.6 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "model.operation=\"classify\"\n",
    "\n",
    "model.system_prompt = \"\"\"You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
    "\n",
    "You are presented with the medical history of a patient and an accompanying image related to the patient. \n",
    "\n",
    "You will be asked a multiple question and are required to select from one of the available answers.\n",
    "\n",
    "Once you have the answer, you  must respond in the following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"classifications\": [\n",
    "        {\n",
    "            \"label\": \"your single letter answer to the question\",\n",
    "            ...,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Always return your response as valid JSON wrapped in ```json blocks and respond only with a one letter answer (one of A, B, C, D, E).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dataset.apply_model(\n",
    "    model, \n",
    "    label_field=\"model_answer\", \n",
    "    prompt_field=\"question\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MedGemma for VQA\n",
    "\n",
    "You can use the model for visual question answering as shown below. This example will use the same prompt on each [Sample](https://docs.voxel51.com/api/fiftyone.core.sample.html#module-fiftyone.core.sample) in the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------------|  0/10 [2.5ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aeb                                            \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aec\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aed\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aee\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aef\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af0\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af1\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  70% |█████████████/-----|  7/10 [149.5ms elapsed, 64.1ms remaining, 46.8 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707af2                                                      \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af3\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af4\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      " 100% |███████████████████| 10/10 [230.5ms elapsed, 0s remaining, 43.4 samples/s]     \n"
     ]
    }
   ],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "model.prompt = \"Describe the condition the image you see here\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the default system prompt, which you can inspect as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have expert-level medical knowledge in radiology, histopathology, ophthalmology, and dermatology.\n",
      "\n",
      "You are presented with the medical history of a patient and an accompanying image related to the patient. \n",
      "\n",
      "You will be asked a multiple question and are required to select from one of the available answers.\n",
      "\n",
      "Once you have the answer, you  must respond in the following format:\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"classifications\": [\n",
      "        {\n",
      "            \"label\": \"your single letter answer to the question\",\n",
      "            ...,\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```\n",
      "\n",
      "Always return your response as valid JSON wrapped in ```json blocks and respond only with a one letter answer (one of A, B, C, D, E).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the system prompt as shown above, by simply following this pattern:\n",
    "\n",
    "```python\n",
    "model.system_prompt = \"An awesome customized system prompt\"\n",
    "```\n",
    "\n",
    "You can use a [Field](https://docs.voxel51.com/api/fiftyone.core.fields.html#module-fiftyone.core.fields) from the sample as a prompt by passing in `prompt_field=<field-name>` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||------------------|  0/10 [2.5ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707aeb                                            \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aec\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aed\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aee\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707aef\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af0\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af1\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "  70% |█████████████/-----|  7/10 [141.5ms elapsed, 60.7ms remaining, 49.5 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n",
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: 682f692778c24a0e8e707af2                                                      \n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af3\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      "Sample: 682f692778c24a0e8e707af4\n",
      "Error: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got CUDABFloat16Type instead (while checking arguments for embedding)\n",
      "\n",
      " 100% |███████████████████| 10/10 [217.3ms elapsed, 0s remaining, 46.0 samples/s]     \n"
     ]
    }
   ],
   "source": [
    "model.operation=\"vqa\"\n",
    "\n",
    "dataset.apply_model(model, label_field=\"verbose_ans\", prompt_field=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "\n",
      "Welcome to\n",
      "\n",
      "███████╗██╗███████╗████████╗██╗   ██╗ ██████╗ ███╗   ██╗███████╗\n",
      "██╔════╝██║██╔════╝╚══██╔══╝╚██╗ ██╔╝██╔═══██╗████╗  ██║██╔════╝\n",
      "█████╗  ██║█████╗     ██║    ╚████╔╝ ██║   ██║██╔██╗ ██║█████╗\n",
      "██╔══╝  ██║██╔══╝     ██║     ╚██╔╝  ██║   ██║██║╚██╗██║██╔══╝\n",
      "██║     ██║██║        ██║      ██║   ╚██████╔╝██║ ╚████║███████╗\n",
      "╚═╝     ╚═╝╚═╝        ╚═╝      ╚═╝    ╚═════╝ ╚═╝  ╚═══╝╚══════╝ v1.7.0\n",
      "\n",
      "If you're finding FiftyOne helpful, here's how you can get involved:\n",
      "\n",
      "|\n",
      "|  ⭐⭐⭐ Give the project a star on GitHub ⭐⭐⭐\n",
      "|  https://github.com/voxel51/fiftyone\n",
      "|\n",
      "|  🚀🚀🚀 Join the FiftyOne Discord community 🚀🚀🚀\n",
      "|  https://community.voxel51.com/\n",
      "|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Launch FiftyOne App with the filtered dataset\n",
    "session = fo.launch_app(dataset, port=5151, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        MedXpertQA\n",
      "Media type:  image\n",
      "Num samples: 10\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    question_id:      fiftyone.core.fields.StringField\n",
      "    question:         fiftyone.core.fields.StringField\n",
      "    options:          fiftyone.core.fields.DictField\n",
      "    label:            fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    medical_task:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    body_system:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "    question_type:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n"
     ]
    }
   ],
   "source": [
    "dataset.persistent=True\n",
    "print(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "health_care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
